{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2abee026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from decimal import Decimal, InvalidOperation\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "pd.options.display.float_format = '{:,.14f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00ed5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações\n",
    "load_dotenv()\n",
    "TOKEN_URL = 'https://api.ticket360.com.br/auth/oauth/access_token'\n",
    "API_URL = 'https://api.ticket360.com.br'\n",
    "EVENT_ID = '30642'\n",
    "MAX_RETRIES = 3  #Max request until fail\n",
    "TIMEOUT = 30  # Seconds\n",
    "DATA_FILE = 'ticket_data.json'\n",
    "\n",
    "#get token from retries and timout adjustaded\n",
    "def get_access_token():\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            auth_string = f\"{os.getenv('CLIENT_ID')}:{os.getenv('CLIENT_SECRET')}\"\n",
    "            auth_base64 = base64.b64encode(auth_string.encode()).decode()\n",
    "            \n",
    "            response = requests.post(\n",
    "                TOKEN_URL,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Basic {auth_base64}\",\n",
    "                    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "                },\n",
    "                data={\"grant_type\": \"client_credentials\"},\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"access_token\")\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout (attempt {attempt + 1}/{MAX_RETRIES})\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            raise \n",
    "        except Exception as e:\n",
    "            print(f\"Erro: {type(e).__name__} - {str(e)}\")\n",
    "            return None\n",
    "\n",
    "#get date from url_api with error handling\n",
    "# Função fetch_report com paginação e filtros de data\n",
    "def fetch_report(token, start_date=None, end_date=None):\n",
    "    try:\n",
    "        base_url = f\"{API_URL}/sales/reports/consolidated/{EVENT_ID}?filter=status=paid&ticket.status=active\"\n",
    "        \n",
    "        # Adicionar filtros de data se fornecidos\n",
    "        if start_date:\n",
    "            base_url += f\"&startDate={start_date}\"\n",
    "        if end_date:\n",
    "            base_url += f\"&endDate={end_date}\"\n",
    "        \n",
    "        offset = 0\n",
    "        limit = 1000\n",
    "        all_sales = []\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{base_url}&limit={limit}&offset={offset}\"\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            sales = data.get('sales', [])\n",
    "            all_sales.extend(sales)\n",
    "            \n",
    "            print(f\"Offset {offset}: {len(sales)} registros\")\n",
    "            \n",
    "            # Se a quantidade de registros for menor que o limite, chegamos ao fim\n",
    "            if len(sales) < limit:\n",
    "                break\n",
    "                \n",
    "            offset += limit\n",
    "        \n",
    "        return {'sales': all_sales}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Timeout when fetching report\")\n",
    "    except Exception as e:\n",
    "        print(f\"Report error: {type(e).__name__} - {str(e)}\")\n",
    "    return None\n",
    "\n",
    "# Função para formatar datas para o padrão ISO com timezone\n",
    "def format_date(date_obj, end_of_day=False):\n",
    "    tz = pytz.timezone('America/Sao_Paulo')\n",
    "    if end_of_day:\n",
    "        date_obj = date_obj.replace(hour=23, minute=59, second=59)\n",
    "    else:\n",
    "        date_obj = date_obj.replace(hour=0, minute=0, second=0)\n",
    "    return date_obj.astimezone(tz).isoformat()\n",
    "\n",
    "# Função para salvar dados em JSON\n",
    "def save_data(df, filename):\n",
    "    # Se o arquivo já existe, combinamos os dados\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            df_existente = pd.read_json(filename, lines=True)\n",
    "            # Combinar datasets\n",
    "            df_final = pd.concat([df_existente, df])\n",
    "            # Remover duplicatas\n",
    "            df_final = df_final.drop_duplicates(subset='ticket.id', keep='last')\n",
    "        except:\n",
    "            df_final = df\n",
    "    else:\n",
    "        df_final = df\n",
    "    \n",
    "    # Salvar em formato JSON\n",
    "    df_final.to_json(filename, orient='records', lines=True)\n",
    "    return df_final\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e424e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mais recente no arquivo: 2025-08-12 14:36:35-03:00\n",
      "Execução subsequente: coletando dados atualizados\n",
      "Coletando dados de 2025-08-12T00:00:00-03:00 até 2025-08-14T23:59:59.281153-03:00\n",
      "Offset 0: 1000 registros\n",
      "Offset 1000: 1000 registros\n",
      "Offset 2000: 1000 registros\n",
      "Offset 3000: 1000 registros\n",
      "Offset 4000: 1000 registros\n",
      "Offset 5000: 1000 registros\n",
      "Offset 6000: 1000 registros\n",
      "Offset 7000: 1000 registros\n",
      "Offset 8000: 1000 registros\n",
      "Offset 9000: 1000 registros\n",
      "Offset 10000: 1000 registros\n",
      "Offset 11000: 1000 registros\n",
      "Offset 12000: 1000 registros\n",
      "Offset 13000: 1000 registros\n",
      "Offset 14000: 1000 registros\n",
      "Offset 15000: 1000 registros\n",
      "Offset 16000: 1000 registros\n",
      "Offset 17000: 1000 registros\n",
      "Offset 18000: 1000 registros\n",
      "Offset 19000: 1000 registros\n",
      "Offset 20000: 1000 registros\n",
      "Offset 21000: 1000 registros\n",
      "Offset 22000: 1000 registros\n",
      "Offset 23000: 1000 registros\n",
      "Offset 24000: 1000 registros\n",
      "Offset 25000: 1000 registros\n",
      "Offset 26000: 1000 registros\n",
      "Offset 27000: 1000 registros\n",
      "Offset 28000: 1000 registros\n",
      "Offset 29000: 1000 registros\n",
      "Offset 30000: 1000 registros\n",
      "Offset 31000: 1000 registros\n",
      "Offset 32000: 1000 registros\n",
      "Offset 33000: 1000 registros\n",
      "Offset 34000: 325 registros\n",
      "Dados atualizados salvos: 34325 registros\n",
      "Processo concluído\n"
     ]
    }
   ],
   "source": [
    "# Fluxo principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurar timezone\n",
    "    tz = pytz.timezone('America/Sao_Paulo')\n",
    "    hoje = datetime.now(tz)\n",
    "    dois_dias_atras = hoje - timedelta(days=2)\n",
    "    ontem = hoje - timedelta(days=1)\n",
    "    \n",
    "    # Obter token\n",
    "    token = get_access_token()\n",
    "    if not token:\n",
    "        print(\"Falha ao obter token. Abortando.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Verificar arquivo existente\n",
    "    df_existente = pd.DataFrame()\n",
    "    data_mais_recente = None\n",
    "    \n",
    "    if os.path.exists(DATA_FILE):\n",
    "        try:\n",
    "            df_existente = pd.read_json(DATA_FILE, lines=True)\n",
    "            # Converter coluna de datas\n",
    "            df_existente['date'] = pd.to_datetime(df_existente['date'])\n",
    "            # Encontrar data mais recente\n",
    "            if not df_existente.empty:\n",
    "                data_mais_recente = df_existente['date'].max().to_pydatetime()\n",
    "                data_mais_recente = data_mais_recente.astimezone(tz)\n",
    "                print(f\"Data mais recente no arquivo: {data_mais_recente}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler arquivo: {e}\")\n",
    "    \n",
    "    # Primeira execução: coletar todo o histórico até 2 dias atrás\n",
    "    if data_mais_recente is None:\n",
    "        print(\"Primeira execução: coletando todo o histórico até 2 dias atrás\")\n",
    "        fim_historico = format_date(dois_dias_atras, end_of_day=True)\n",
    "        report = fetch_report(token, end_date=fim_historico)\n",
    "        \n",
    "        if report and 'sales' in report:\n",
    "            df_historico = pd.DataFrame(report['sales'])\n",
    "            if not df_historico.empty:\n",
    "                # Converter datas\n",
    "                df_historico['date'] = pd.to_datetime(df_historico['date'])\n",
    "                # Salvar dados históricos\n",
    "                df_final = save_data(df_historico, DATA_FILE)\n",
    "                print(f\"Dados históricos salvos: {len(df_final)} registros\")\n",
    "            else:\n",
    "                print(\"Nenhum dado histórico encontrado\")\n",
    "        else:\n",
    "            print(\"Falha ao coletar dados históricos\")\n",
    "    else:\n",
    "        # Execuções subsequentes: coletar dados desde a última data até ontem\n",
    "        print(\"Execução subsequente: coletando dados atualizados\")\n",
    "        \n",
    "        # Coletar dados desde a última data até ontem\n",
    "        inicio_atualizacao = format_date(data_mais_recente + timedelta(seconds=1))\n",
    "        fim_atualizacao = format_date(ontem, end_of_day=True)\n",
    "        \n",
    "        print(f\"Coletando dados de {inicio_atualizacao} até {fim_atualizacao}\")\n",
    "        report = fetch_report(token, start_date=inicio_atualizacao, end_date=fim_atualizacao)\n",
    "        \n",
    "        if report and 'sales' in report:\n",
    "            df_novos = pd.DataFrame(report['sales'])\n",
    "            if not df_novos.empty:\n",
    "                # Converter datas\n",
    "                df_novos['date'] = pd.to_datetime(df_novos['date'])\n",
    "                # Salvar novos dados\n",
    "                df_final = save_data(df_novos, DATA_FILE)\n",
    "                print(f\"Dados atualizados salvos: {len(df_final)} registros\")\n",
    "            else:\n",
    "                print(\"Nenhum dado novo encontrado no período\")\n",
    "        else:\n",
    "            print(\"Falha ao coletar dados atualizados\")\n",
    "    \n",
    "    print(\"Processo concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e90960",
   "metadata": {},
   "source": [
    "<h1>Code below is used for validate JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84c2bb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noffset_json = \"ticket_data.json\"\\npath_json = os.path.join(os.getcwd(), offset_json)\\n\\ndf = pd.read_json(path_json, lines=True)\\nprint(df.sort_values(by=\\'date\\', ascending=False).head())\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "offset_json = \"ticket_data.json\"\n",
    "path_json = os.path.join(os.getcwd(), offset_json)\n",
    "\n",
    "df = pd.read_json(path_json, lines=True)\n",
    "print(df.sort_values(by='date', ascending=False).head())\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
