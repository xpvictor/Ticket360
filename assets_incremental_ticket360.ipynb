{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ee9db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_25868\\393310270.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from decimal import Decimal, InvalidOperation\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:,.14f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29df5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "TOKEN_URL = 'https://api.ticket360.com.br/auth/oauth/access_token'\n",
    "API_URL = 'https://api.ticket360.com.br'\n",
    "EVENT_ID = '30617'\n",
    "EVENT_NAME = 'Camarote-essepe-2026-grupo-especial-thiaguinho'\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 30\n",
    "DATA_DIR = \"ticket_data\"\n",
    "CONSOLIDATED_FILE = os.path.join(DATA_DIR, \"consolidated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c938a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    \"\"\"Get access token with treatment errors\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            auth_string = f\"{os.getenv('CLIENT_ID')}:{os.getenv('CLIENT_SECRET')}\"\n",
    "            auth_base64 = base64.b64encode(auth_string.encode()).decode()\n",
    "            \n",
    "            response = requests.post(\n",
    "                TOKEN_URL,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Basic {auth_base64}\",\n",
    "                    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "                },\n",
    "                data={\"grant_type\": \"client_credentials\"},\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"access_token\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout (tentativa {attempt + 1}/{MAX_RETRIES})\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error to get token: {type(e).__name__} - {str(e)}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1dbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_report(token, start_date=None, end_date=None):\n",
    "    '''Search report from API with page and aplly filter conditions'''\n",
    "    try:\n",
    "        base_url = f\"{API_URL}/sales/reports/consolidated/{EVENT_ID}?filter=status=paid&ticket.status=active\"\n",
    "        \n",
    "        offset = 0\n",
    "        limit = 1000\n",
    "        all_sales = []\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{base_url}&limit={limit}&offset={offset}\"\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            sales = data.get('sales', [])\n",
    "            all_sales.extend(sales)\n",
    "            \n",
    "            if len(sales) < limit:\n",
    "                break\n",
    "            offset += limit\n",
    "        \n",
    "        # Convert to dataframe with error treatment\n",
    "        df_sales = pd.DataFrame(all_sales)\n",
    "        if \"date\" in df_sales.columns and not df_sales.empty:\n",
    "            #If the column is numerical then timestamp in ms\n",
    "            if pd.api.types.is_numeric_dtype(df_sales[\"date\"]):\n",
    "                df_sales[\"date\"] = pd.to_datetime(df_sales[\"date\"], errors=\"coerce\", unit=\"ms\")\n",
    "            else:\n",
    "                # If the column is already ISO\n",
    "                df_sales[\"date\"] = pd.to_datetime(df_sales[\"date\"], errors=\"coerce\")\n",
    "            \n",
    "            # Apply filter to start_date/end_date if already existis\n",
    "            if start_date:\n",
    "                df_sales = df_sales[df_sales[\"date\"].dt.date >= pd.to_datetime(start_date).date()]\n",
    "            if end_date:\n",
    "                df_sales = df_sales[df_sales[\"date\"].dt.date <= pd.to_datetime(end_date).date()]\n",
    "        \n",
    "        return {\"sales\": df_sales.to_dict(orient=\"records\")}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error to fetch the report: {type(e).__name__} - {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dates(df: pd.DataFrame, date_column: str = \"date\") -> pd.DataFrame:\n",
    "    ''''Normalize data to ISO-8601 format (YYYY-MM-DDTHH:MM:SS±HH:MM)'''\n",
    "    if date_column not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True, errors=\"coerce\")\n",
    "    \n",
    "    # Convert to string in ISO format\n",
    "    df[date_column] = df[date_column].dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    \n",
    "    df[date_column] = df[date_column].str.replace(\n",
    "        r\"(\\+)(\\d{2})(\\d{2})$\", r\"\\1\\2:\\3\", regex=True\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9490eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_today_data(df):\n",
    "    hoje = datetime.now().date()\n",
    "\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "        df[\"date_only\"] = df[\"date\"].dt.date\n",
    "        df = df[df[\"date_only\"] != hoje].drop(columns=[\"date_only\"])\n",
    "    else:\n",
    "        print(\"'Date' column not found\")\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_consolidated_ndjson(filepath):\n",
    "    # Read data from JSON file in array format\n",
    "    if not os.path.exists(filepath):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            elif isinstance(data, dict):\n",
    "                return [data] \n",
    "            else:\n",
    "                print(f\"Unnexpected format at file {filepath}\")\n",
    "                return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error to load {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e86935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_consolidated_json(filepath, new_records):\n",
    "    #Add new records to consolidated.json file keeping the array format\n",
    "    existing_data = load_consolidated_ndjson(filepath)\n",
    "    updated_data = existing_data + new_records\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(updated_data, f, indent=2, ensure_ascii=False, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c610bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_date_from_consolidated():\n",
    "    records = load_consolidated_ndjson(CONSOLIDATED_FILE)\n",
    "    if not records:\n",
    "        return None\n",
    "    df = pd.DataFrame(records)\n",
    "    if \"date\" not in df.columns or df.empty:\n",
    "        return None\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "    return df[\"date\"].max().date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    '''Save the data in JSON file with data treatment'''\n",
    "    def convert_timestamps(obj):\n",
    "        if isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        return obj\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            json_data = data.to_dict(orient='records')\n",
    "            json.dump(json_data, f, indent=2, default=convert_timestamps)\n",
    "        else:\n",
    "            json.dump(data, f, indent=2, default=convert_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ee18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data(new_data_file):\n",
    "    ''''Append new data to main file \"consolidated.json\"'''\n",
    "    # Carregar dados existentes\n",
    "    df_new = pd.read_json(CONSOLIDATED_FILE)\n",
    "\n",
    "    if os.path.exists(CONSOLIDATED_FILE) and os.path.getsize(CONSOLIDATED_FILE) > 2:\n",
    "        try:\n",
    "            df_old = pd.read_json(CONSOLIDATED_FILE)\n",
    "        except ValueError:\n",
    "            with open(CONSOLIDATED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw = f.read()\n",
    "            if \"]\" in raw:\n",
    "                raw = raw[:raw.rfind(\"]\")+1]\n",
    "                df_old = pd.read_json(pd.io.common.StringIO(raw))\n",
    "            else:\n",
    "                df_old = pd.DataFrame()\n",
    "    else:\n",
    "        df_old = pd.DataFrame()\n",
    "\n",
    "    if not df_old.empty:\n",
    "        df_all = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_all = df_new.copy()\n",
    "\n",
    "    #Remove duplicates\n",
    "    keys_prioridade = ['ticket.id', 'ticket.code', 'id']\n",
    "    subset = [c for c in keys_prioridade if c in df_all.columns]\n",
    "    if subset:\n",
    "        if 'date' in df_all.columns:\n",
    "            df_all = df_all.sort_values('date')\n",
    "        df_all = df_all.drop_duplicates(subset=subset, keep='last')\n",
    "\n",
    "    df_all.to_json(CONSOLIDATED_FILE, orient='records', force_ascii=False, indent=2)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3818e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_date():\n",
    "    '''Get the latest data from consolidated.json file'''\n",
    "    if not os.path.exists(CONSOLIDATED_FILE):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_json(CONSOLIDATED_FILE)\n",
    "        if 'date' not in df.columns or df.empty:\n",
    "            return None\n",
    "        if pd.api.types.is_integer_dtype(df['date']):\n",
    "            s = pd.to_datetime(df['date'], unit='ms', utc=True)\n",
    "        else:\n",
    "            s = pd.to_datetime(df['date'],errors='coerce',utc=True)\n",
    "\n",
    "        if s.isna().all():\n",
    "            return None \n",
    "        \n",
    "        return s.max().date()\n",
    "    except Exception as e:\n",
    "        print(f\"Error to get last date: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccdfe02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_incremental_file(data, prefix=\"incremental\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.join(DATA_DIR, f\"{prefix}_{timestamp}.json\")\n",
    "    save_to_json(data, filename)\n",
    "    print(f\"Incremental data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ff8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_incremental_pipeline():\n",
    "    token = get_access_token()\n",
    "    if not token:\n",
    "        print(\"Error to get token\")\n",
    "        return \n",
    "    \n",
    "    # Requisição à API\n",
    "    report_data = fetch_report(token)\n",
    "    if not report_data or \"sales\" not in report_data:\n",
    "        print(\"No data returned from API\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(report_data[\"sales\"])\n",
    "    if df.empty:\n",
    "        print(\"No sales data found\")\n",
    "        return\n",
    "    \n",
    "    df = normalize_dates(df, date_column=\"date\")\n",
    "\n",
    "    last_date = get_last_date_from_consolidated()   # Most updated data alredy saved\n",
    "    if last_date is None:\n",
    "        print(\"No consolidated data found\")\n",
    "        return\n",
    "    \n",
    "    start_date = last_date + timedelta(days=1)     \n",
    "    today = datetime.now().date()\n",
    "\n",
    "    # Filter the records after the last_date\n",
    "    df_filtered = df[df[\"date\"].apply(lambda x: pd.to_datetime(x).date() >= start_date)]\n",
    "\n",
    "    # Remover registros de hoje\n",
    "    df_filtered = remove_today_data(df_filtered)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(\"Already up to date (no new data)\")\n",
    "        return\n",
    "    \n",
    "    # Oldest date avaliable into df_filtered\n",
    "    min_date = df_filtered[\"date\"].apply(lambda x: pd.to_datetime(x).date()).min()\n",
    "\n",
    "    if min_date == today:\n",
    "        print(\"Already up to date (only today’s data available)\")\n",
    "        return\n",
    "    \n",
    "    if min_date > last_date and min_date != today:\n",
    "        # Append ao consolidated.json\n",
    "        append_to_consolidated_json(CONSOLIDATED_FILE, df_filtered.to_dict(orient=\"records\"))\n",
    "        # Save incremental data into new json filte\n",
    "        save_incremental_file(df_filtered)\n",
    "        print(f\"Incremental file created: {CONSOLIDATED_FILE} ({len(df_filtered)} records)\")\n",
    "    else:\n",
    "        print(\"Already up to date (no valid new data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39e3e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental data saved to ticket_data\\incremental_20250908_103132.json\n",
      "Incremental file created: ticket_data\\consolidated.json (28 records)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_25868\\3889756053.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_25868\\3889756053.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"date_only\"] = df[\"date\"].dt.date\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_incremental_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
