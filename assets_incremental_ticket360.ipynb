{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4ee9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from decimal import Decimal, InvalidOperation\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:,.14f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d29df5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "TOKEN_URL = 'https://api.ticket360.com.br/auth/oauth/access_token'\n",
    "API_URL = 'https://api.ticket360.com.br'\n",
    "EVENT_ID = '30617'\n",
    "EVENT_NAME = 'Camarote-essepe-2026-grupo-especial-thiaguinho'\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 30\n",
    "DATA_DIR = \"ticket_data\"\n",
    "CONSOLIDATED_FILE = os.path.join(DATA_DIR, \"consolidated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c938a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    \"\"\"Obtém token de acesso com tratamento de erros\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            auth_string = f\"{os.getenv('CLIENT_ID')}:{os.getenv('CLIENT_SECRET')}\"\n",
    "            auth_base64 = base64.b64encode(auth_string.encode()).decode()\n",
    "            \n",
    "            response = requests.post(\n",
    "                TOKEN_URL,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Basic {auth_base64}\",\n",
    "                    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "                },\n",
    "                data={\"grant_type\": \"client_credentials\"},\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"access_token\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout (tentativa {attempt + 1}/{MAX_RETRIES})\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao obter token: {type(e).__name__} - {str(e)}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e1dbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_report(token, start_date=None, end_date=None):\n",
    "    \"\"\"Busca relatório da API com paginação e aplica filtro local de datas\"\"\"\n",
    "    try:\n",
    "        base_url = f\"{API_URL}/sales/reports/consolidated/{EVENT_ID}?filter=status=paid&ticket.status=active\"\n",
    "        \n",
    "        offset = 0\n",
    "        limit = 1000\n",
    "        all_sales = []\n",
    "        \n",
    "        while True:\n",
    "            url = f\"{base_url}&limit={limit}&offset={offset}\"\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            sales = data.get('sales', [])\n",
    "            all_sales.extend(sales)\n",
    "            \n",
    "            if len(sales) < limit:\n",
    "                break\n",
    "            offset += limit\n",
    "        \n",
    "        # Converte para DataFrame para tratamento de datas\n",
    "        df_sales = pd.DataFrame(all_sales)\n",
    "        if \"date\" in df_sales.columns and not df_sales.empty:\n",
    "            # Se a coluna for numérica -> timestamp em ms\n",
    "            if pd.api.types.is_numeric_dtype(df_sales[\"date\"]):\n",
    "                df_sales[\"date\"] = pd.to_datetime(df_sales[\"date\"], errors=\"coerce\", unit=\"ms\")\n",
    "            else:\n",
    "                # Se já vier como string ISO\n",
    "                df_sales[\"date\"] = pd.to_datetime(df_sales[\"date\"], errors=\"coerce\")\n",
    "            \n",
    "            # Se existirem filtros de start_date / end_date, aplicar\n",
    "            if start_date:\n",
    "                df_sales = df_sales[df_sales[\"date\"].dt.date >= pd.to_datetime(start_date).date()]\n",
    "            if end_date:\n",
    "                df_sales = df_sales[df_sales[\"date\"].dt.date <= pd.to_datetime(end_date).date()]\n",
    "        \n",
    "        return {\"sales\": df_sales.to_dict(orient=\"records\")}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao buscar relatório: {type(e).__name__} - {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4db8206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dates(df: pd.DataFrame, date_column: str = \"date\") -> pd.DataFrame:\n",
    "    \"\"\"Normaliza datas para o formato ISO-8601 (YYYY-MM-DDTHH:MM:SS±HH:MM).\"\"\"\n",
    "    if date_column not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    # Converte para datetime, forçando UTC quando possível\n",
    "    df[date_column] = pd.to_datetime(df[date_column], utc=True, errors=\"coerce\")\n",
    "    \n",
    "    # Converte para string no formato ISO\n",
    "    df[date_column] = df[date_column].dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    \n",
    "    # Ajusta fuso horário para ter o \":\" no offset (+0000 -> +00:00)\n",
    "    df[date_column] = df[date_column].str.replace(\n",
    "        r\"(\\+)(\\d{2})(\\d{2})$\", r\"\\1\\2:\\3\", regex=True\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9490eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_today_data(df):\n",
    "    hoje = datetime.now().date()\n",
    "\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "        df[\"date_only\"] = df[\"date\"].dt.date\n",
    "        df = df[df[\"date_only\"] != hoje].drop(columns=[\"date_only\"])\n",
    "    else:\n",
    "        print(\"'Date' column not found\")\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccab8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_consolidated_ndjson(filepath):\n",
    "    # Lê dados de um arquivo JSON no formato array\n",
    "    if not os.path.exists(filepath):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            elif isinstance(data, dict):\n",
    "                return [data]  # caso o arquivo tenha apenas um registro\n",
    "            else:\n",
    "                print(f\"Formato inesperado no arquivo {filepath}\")\n",
    "                return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erro ao carregar {filepath}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e86935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_consolidated_json(filepath, new_records):\n",
    "    #Add new records to consolidated.json file keeping the array format\n",
    "    existing_data = load_consolidated_ndjson(filepath)\n",
    "    updated_data = existing_data + new_records\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(updated_data, f, indent=2, ensure_ascii=False, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c610bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_date_from_consolidated():\n",
    "    records = load_consolidated_ndjson(CONSOLIDATED_FILE)\n",
    "    if not records:\n",
    "        return None\n",
    "    df = pd.DataFrame(records)\n",
    "    if \"date\" not in df.columns or df.empty:\n",
    "        return None\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "    return df[\"date\"].max().date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf6e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    \"\"\"Salva dados em um arquivo JSON com tratamento de datas\"\"\"\n",
    "    def convert_timestamps(obj):\n",
    "        if isinstance(obj, pd.Timestamp):\n",
    "            return obj.isoformat()\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        return obj\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            json_data = data.to_dict(orient='records')\n",
    "            json.dump(json_data, f, indent=2, default=convert_timestamps)\n",
    "        else:\n",
    "            json.dump(data, f, indent=2, default=convert_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b5ee18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data(new_data_file):\n",
    "    \"\"\"Consolida novos dados com o arquivo principal\"\"\"\n",
    "    # Carregar dados existentes\n",
    "    df_new = pd.read_json(CONSOLIDATED_FILE)\n",
    "\n",
    "    if os.path.exists(CONSOLIDATED_FILE) and os.path.getsize(CONSOLIDATED_FILE) > 2:\n",
    "        try:\n",
    "            df_old = pd.read_json(CONSOLIDATED_FILE)\n",
    "        except ValueError:\n",
    "            # Se estiver corrompido, tenta reparar removendo qualquer “lixo” após o colchete\n",
    "            with open(CONSOLIDATED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw = f.read()\n",
    "            if \"]\" in raw:\n",
    "                raw = raw[:raw.rfind(\"]\")+1]\n",
    "                df_old = pd.read_json(pd.io.common.StringIO(raw))\n",
    "            else:\n",
    "                df_old = pd.DataFrame()\n",
    "    else:\n",
    "        df_old = pd.DataFrame()\n",
    "\n",
    "    # Concatena\n",
    "    if not df_old.empty:\n",
    "        df_all = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_all = df_new.copy()\n",
    "\n",
    "    # Remoção de duplicatas (ajuste as chaves conforme sua noção de unicidade)\n",
    "    keys_prioridade = ['ticket.id', 'ticket.code', 'id']\n",
    "    subset = [c for c in keys_prioridade if c in df_all.columns]\n",
    "    if subset:\n",
    "        # Mantém o mais recente pela coluna 'date' quando houver\n",
    "        if 'date' in df_all.columns:\n",
    "            df_all = df_all.sort_values('date')\n",
    "        df_all = df_all.drop_duplicates(subset=subset, keep='last')\n",
    "\n",
    "    # NÃO TOCA no tipo de 'date' (se já está em ms, mantém; se já é string, mantém)\n",
    "    # Salva como JSON array bonito\n",
    "    df_all.to_json(CONSOLIDATED_FILE, orient='records', force_ascii=False, indent=2)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3818e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_date():\n",
    "    \"\"\"Obtém a última data do dataset consolidado\"\"\"\n",
    "    if not os.path.exists(CONSOLIDATED_FILE):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_json(CONSOLIDATED_FILE)\n",
    "        if 'date' not in df.columns or df.empty:\n",
    "            return None\n",
    "        if pd.api.types.is_integer_dtype(df['date']):\n",
    "            s = pd.to_datetime(df['date'], unit='ms', utc=True)\n",
    "        else:\n",
    "            s = pd.to_datetime(df['date'],errors='coerce',utc=True)\n",
    "\n",
    "        if s.isna().all():\n",
    "            return None \n",
    "        \n",
    "        return s.max().date()\n",
    "    except Exception as e:\n",
    "        print(f\"Erroe to get last date: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccdfe02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_incremental_file(data, prefix=\"incremental\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = os.path.join(DATA_DIR, f\"{prefix}_{timestamp}.json\")\n",
    "    save_to_json(data, filename)\n",
    "    print(f\"Incremental data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642ff8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_incremental_pipeline():\n",
    "    token = get_access_token()\n",
    "    if not token:\n",
    "        print(\"Error to get token\")\n",
    "        return \n",
    "    \n",
    "    # Requisição à API\n",
    "    report_data = fetch_report(token)\n",
    "    if not report_data or \"sales\" not in report_data:\n",
    "        print(\"No data returned from API\")\n",
    "        return\n",
    "    \n",
    "    # Converter para DataFrame e normalizar\n",
    "    df = pd.DataFrame(report_data[\"sales\"])\n",
    "    if df.empty:\n",
    "        print(\"No sales data found\")\n",
    "        return\n",
    "    \n",
    "    df = normalize_dates(df, date_column=\"date\")\n",
    "\n",
    "    # Variáveis principais\n",
    "    last_date = get_last_date_from_consolidated()   # data mais recente já armazenada\n",
    "    if last_date is None:\n",
    "        print(\"No consolidated data found\")\n",
    "        return\n",
    "    \n",
    "    start_date = last_date + timedelta(days=1)      # coleta deve iniciar a partir do próximo dia\n",
    "    today = datetime.now().date()\n",
    "\n",
    "    # Filtrar registros somente após a last_date\n",
    "    df_filtered = df[df[\"date\"].apply(lambda x: pd.to_datetime(x).date() >= start_date)]\n",
    "\n",
    "    # Remover registros de hoje\n",
    "    df_filtered = remove_today_data(df_filtered)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(\"Already up to date (no new data)\")\n",
    "        return\n",
    "    \n",
    "    # Data mais antiga disponível no df filtrado\n",
    "    min_date = df_filtered[\"date\"].apply(lambda x: pd.to_datetime(x).date()).min()\n",
    "\n",
    "    if min_date == today:\n",
    "        print(\"Already up to date (only today’s data available)\")\n",
    "        return\n",
    "    \n",
    "    if min_date > last_date and min_date != today:\n",
    "        # Append ao consolidated.json\n",
    "        append_to_consolidated_json(CONSOLIDATED_FILE, df_filtered.to_dict(orient=\"records\"))\n",
    "        # Salvar incremental separado\n",
    "        save_incremental_file(df_filtered)\n",
    "        print(f\"Incremental file created: {CONSOLIDATED_FILE} ({len(df_filtered)} records)\")\n",
    "    else:\n",
    "        print(\"Already up to date (no valid new data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39e3e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date (no new data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_16824\\3889756053.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_16824\\3889756053.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"date_only\"] = df[\"date\"].dt.date\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_incremental_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros antes: 782\n",
      "Registros deletados: 2\n",
      "Total de registros depois: 780\n",
      "Arquivo atualizado: ticket_data/consolidated.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def delete_records_by_date(file_path, target_date):\n",
    "    \"\"\"\n",
    "    Deleta registros onde 'date' é igual à data especificada\n",
    "    target_date: string no formato 'YYYY-MM-DD' (ex: '2025-08-31')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ler o arquivo JSON como array\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"Total de registros antes: {len(data)}\")\n",
    "        \n",
    "        # Filtrar registros que NÃO têm a data alvo\n",
    "        filtered_data = []\n",
    "        deleted_count = 0\n",
    "        \n",
    "        for record in data:\n",
    "            record_date = record.get('date', '')\n",
    "            \n",
    "            # Converter para string se for timestamp\n",
    "            if isinstance(record_date, (int, float)):\n",
    "                try:\n",
    "                    dt = datetime.fromtimestamp(record_date / 1000)\n",
    "                    record_date_str = dt.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    record_date_str = ''\n",
    "            else:\n",
    "                record_date_str = str(record_date)\n",
    "            \n",
    "            # Manter apenas registros que NÃO têm a data alvo\n",
    "            if target_date not in record_date_str:\n",
    "                filtered_data.append(record)\n",
    "            else:\n",
    "                deleted_count += 1\n",
    "        \n",
    "        # Salvar o array filtrado de volta no arquivo\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Registros deletados: {deleted_count}\")\n",
    "        print(f\"Total de registros depois: {len(filtered_data)}\")\n",
    "        print(f\"Arquivo atualizado: {file_path}\")\n",
    "        \n",
    "        return deleted_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Exemplo de uso:\n",
    "delete_records_by_date(\"ticket_data/consolidated.json\", \"2025-09-02\")\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
